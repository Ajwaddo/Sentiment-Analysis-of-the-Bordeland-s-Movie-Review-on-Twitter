{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pip install pandas scikit-learn nltk textblob vaderSentiment wordcloud matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('vader_lexicon')\n",
    "    print(\"‚úÖ NLTK data downloaded successfully!\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Some NLTK downloads failed - continuing anyway\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('archive-2/twitter_training.csv')\n",
    "\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA DISTRIBUTION ===\")\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"Sentiment distribution:\")\n",
    "sentiment_counts = df['Sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "print(f\"\\nSentiment percentages:\")\n",
    "print(df['Sentiment'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Entity distribution\n",
    "print(f\"\\nEntity distribution:\")\n",
    "entity_counts = df['Entity'].value_counts()\n",
    "print(entity_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization - Sentiment and Entity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts.plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Sentiment Distribution')\n",
    "axes[0,0].set_xlabel('Sentiment')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Sentiment pie chart\n",
    "sentiment_counts.plot(kind='pie', ax=axes[0,1], autopct='%1.1f%%')\n",
    "axes[0,1].set_title('Sentiment Distribution (Pie Chart)')\n",
    "axes[0,1].set_ylabel('')\n",
    "\n",
    "# Entity distribution (top 10)\n",
    "entity_counts.head(10).plot(kind='bar', ax=axes[1,0], color='lightcoral')\n",
    "axes[1,0].set_title('Top 10 Entities')\n",
    "axes[1,0].set_xlabel('Entity')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Entity-Sentiment heatmap\n",
    "entity_sentiment = pd.crosstab(df['Entity'], df['Sentiment'])\n",
    "sns.heatmap(entity_sentiment, annot=True, fmt='d', cmap='Blues', ax=axes[1,1])\n",
    "axes[1,1].set_title('Entity-Sentiment Heatmap')\n",
    "axes[1,1].set_xlabel('Sentiment')\n",
    "axes[1,1].set_ylabel('Entity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text, use_stemming=True, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing function\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags (but keep the text)\n",
    "    text = re.sub(r'@\\w+|#', '', text)\n",
    "    \n",
    "    # Remove special characters and digits, keep only letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords if specified\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming if specified\n",
    "    if use_stemming:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Remove empty tokens and join\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = df['Tweet Content'].iloc[0]\n",
    "print(\"Original text:\", sample_text)\n",
    "print(\"Preprocessed text:\", preprocess_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzers\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def add_engineered_features(df):\n",
    "    \"\"\"\n",
    "    Add engineered features to the dataframe\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Preprocess text\n",
    "    df['processed_text'] = df['Tweet Content'].apply(\n",
    "        lambda x: preprocess_text(x, use_stemming=True, remove_stopwords=True)\n",
    "    )\n",
    "    \n",
    "    # Basic text features\n",
    "    df['text_length'] = df['Tweet Content'].astype(str).apply(len)\n",
    "    df['word_count'] = df['Tweet Content'].astype(str).apply(lambda x: len(x.split()))\n",
    "    df['processed_word_count'] = df['processed_text'].apply(lambda x: len(x.split()) if x else 0)\n",
    "    \n",
    "    # TextBlob sentiment features\n",
    "    df['textblob_polarity'] = df['Tweet Content'].astype(str).apply(\n",
    "        lambda x: TextBlob(x).sentiment.polarity\n",
    "    )\n",
    "    df['textblob_subjectivity'] = df['Tweet Content'].astype(str).apply(\n",
    "        lambda x: TextBlob(x).sentiment.subjectivity\n",
    "    )\n",
    "    \n",
    "    # VADER sentiment features\n",
    "    vader_scores = df['Tweet Content'].astype(str).apply(\n",
    "        lambda x: vader_analyzer.polarity_scores(x)\n",
    "    )\n",
    "    df['vader_compound'] = vader_scores.apply(lambda x: x['compound'])\n",
    "    df['vader_pos'] = vader_scores.apply(lambda x: x['pos'])\n",
    "    df['vader_neu'] = vader_scores.apply(lambda x: x['neu'])\n",
    "    df['vader_neg'] = vader_scores.apply(lambda x: x['neg'])\n",
    "    \n",
    "    # Punctuation and style features\n",
    "    df['exclamation_count'] = df['Tweet Content'].astype(str).apply(lambda x: x.count('!'))\n",
    "    df['question_count'] = df['Tweet Content'].astype(str).apply(lambda x: x.count('?'))\n",
    "    df['capital_ratio'] = df['Tweet Content'].astype(str).apply(\n",
    "        lambda x: sum(1 for c in x if c.isupper()) / max(len(x), 1)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = add_engineered_features(df)\n",
    "\n",
    "print(\"‚úÖ Feature engineering completed!\")\n",
    "print(f\"New features added: {list(df_features.columns[len(df.columns):])}\")\n",
    "print(f\"Dataset shape after feature engineering: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Analysis and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for analysis\n",
    "numerical_features = ['text_length', 'word_count', 'processed_word_count',\n",
    "                     'textblob_polarity', 'textblob_subjectivity',\n",
    "                     'vader_compound', 'vader_pos', 'vader_neu', 'vader_neg',\n",
    "                     'exclamation_count', 'question_count', 'capital_ratio']\n",
    "\n",
    "print(\"=== FEATURE ANALYSIS ===\")\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(df_features[numerical_features].describe())\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_features[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment-based Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis by sentiment\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "features_to_plot = ['textblob_polarity', 'vader_compound', 'text_length', \n",
    "                   'exclamation_count', 'question_count', 'capital_ratio']\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    sns.boxplot(data=df_features, x='Sentiment', y=feature, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by Sentiment')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df_features['Sentiment'].unique()\n",
    "n_sentiments = len(sentiments)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_sentiments, figsize=(5 * n_sentiments, 5))\n",
    "if n_sentiments == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    sentiment_text = ' '.join(df_features[df_features['Sentiment'] == sentiment]['processed_text'].astype(str))\n",
    "    \n",
    "    if sentiment_text.strip():\n",
    "        wordcloud = WordCloud(width=400, height=400, \n",
    "                            background_color='white',\n",
    "                            colormap='viridis').generate(sentiment_text)\n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].set_title(f'{sentiment} Sentiment Word Cloud')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "text_features = df_features['processed_text']\n",
    "numerical_features_list = ['text_length', 'word_count', 'textblob_polarity', 'textblob_subjectivity',\n",
    "                          'vader_compound', 'vader_pos', 'vader_neu', 'vader_neg',\n",
    "                          'exclamation_count', 'question_count', 'capital_ratio']\n",
    "numerical_features = df_features[numerical_features_list]\n",
    "target = df_features['Sentiment']\n",
    "\n",
    "print(\"=== DATA PREPARATION ===\")\n",
    "print(f\"Text features shape: {text_features.shape}\")\n",
    "print(f\"Numerical features shape: {numerical_features.shape}\")\n",
    "print(f\"Target shape: {target.shape}\")\n",
    "print(f\"Target classes: {target.unique()}\")\n",
    "\n",
    "# Split the data\n",
    "X_text_train, X_text_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
    "    text_features, numerical_features, target, \n",
    "    test_size=0.2, random_state=42, stratify=target\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {len(X_text_train)}\")\n",
    "print(f\"Test set size: {len(X_text_test)}\")\n",
    "print(f\"Train set sentiment distribution:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizers\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2)\n",
    "count_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2)\n",
    "\n",
    "# Fit and transform text data\n",
    "print(\"=== TEXT VECTORIZATION ===\")\n",
    "print(\"Creating TF-IDF vectors...\")\n",
    "X_text_tfidf_train = tfidf_vectorizer.fit_transform(X_text_train)\n",
    "X_text_tfidf_test = tfidf_vectorizer.transform(X_text_test)\n",
    "\n",
    "print(\"Creating Count vectors...\")\n",
    "X_text_count_train = count_vectorizer.fit_transform(X_text_train)\n",
    "X_text_count_test = count_vectorizer.transform(X_text_test)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_text_tfidf_train.shape}\")\n",
    "print(f\"Count feature matrix shape: {X_text_count_train.shape}\")\n",
    "\n",
    "# Combine text and numerical features\n",
    "print(\"\\nCombining text and numerical features...\")\n",
    "X_combined_tfidf_train = hstack([X_text_tfidf_train, X_num_train.values])\n",
    "X_combined_tfidf_test = hstack([X_text_tfidf_test, X_num_test.values])\n",
    "\n",
    "X_combined_count_train = hstack([X_text_count_train, X_num_train.values])\n",
    "X_combined_count_test = hstack([X_text_count_test, X_num_test.values])\n",
    "\n",
    "print(f\"Combined TF-IDF features shape: {X_combined_tfidf_train.shape}\")\n",
    "print(f\"Combined Count features shape: {X_combined_count_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Naive Bayes': MultinomialNB()\n",
    "}\n",
    "\n",
    "# Define feature combinations\n",
    "feature_combinations = {\n",
    "    'TF-IDF + Features': (X_combined_tfidf_train, X_combined_tfidf_test),\n",
    "    'Count + Features': (X_combined_count_train, X_combined_count_test),\n",
    "    'TF-IDF Only': (X_text_tfidf_train, X_text_tfidf_test),\n",
    "    'Count Only': (X_text_count_train, X_text_count_test)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "print(\"=== MODEL TRAINING AND EVALUATION ===\")\n",
    "print(\"Training models...\")\n",
    "\n",
    "# Calculate total combinations for overall progress\n",
    "total_combinations = len(feature_combinations) * len(models)\n",
    "\n",
    "# Create overall progress bar\n",
    "with tqdm(total=total_combinations, desc=\"Overall Progress\", unit=\"model\") as pbar_overall:\n",
    "    \n",
    "    # Iterate through feature combinations\n",
    "    for feature_name, (X_train, X_test) in feature_combinations.items():\n",
    "        print(f\"\\n--- {feature_name} ---\")\n",
    "        \n",
    "        # Create progress bar for current feature combination\n",
    "        with tqdm(models.items(), desc=f\"Training {feature_name}\", leave=False) as pbar_models:\n",
    "            \n",
    "            for model_name, model in pbar_models:\n",
    "                # Update the description to show current model\n",
    "                pbar_models.set_description(f\"Training {model_name}\")\n",
    "                \n",
    "                # Train model\n",
    "                start_time = time.time()\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics with progress for cross-validation\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                # Cross-validation with progress (if you want to show CV folds)\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "                cv_mean = cv_scores.mean()\n",
    "                cv_std = cv_scores.std()\n",
    "                \n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "                # Store results\n",
    "                key = f\"{feature_name}_{model_name}\"\n",
    "                results[key] = {\n",
    "                    'model': model,\n",
    "                    'feature_type': feature_name,\n",
    "                    'accuracy': accuracy,\n",
    "                    'cv_mean': cv_mean,\n",
    "                    'cv_std': cv_std,\n",
    "                    'y_pred': y_pred,\n",
    "                    'training_time': training_time\n",
    "                }\n",
    "                \n",
    "                # Update progress bar postfix with current results\n",
    "                pbar_models.set_postfix({\n",
    "                    'Acc': f\"{accuracy:.3f}\",\n",
    "                    'CV': f\"{cv_mean:.3f}\",\n",
    "                    'Time': f\"{training_time:.1f}s\"\n",
    "                })\n",
    "                \n",
    "                print(f\"  {model_name}: Accuracy: {accuracy:.4f}, CV: {cv_mean:.4f} (+/- {cv_std * 2:.4f}), Time: {training_time:.2f}s\")\n",
    "                \n",
    "                # Update overall progress\n",
    "                pbar_overall.update(1)\n",
    "                pbar_overall.set_postfix({\n",
    "                    'Current': f\"{feature_name[:8]}+{model_name[:8]}\",\n",
    "                    'Best_Acc': f\"{max([r['accuracy'] for r in results.values()]):.3f}\"\n",
    "                })\n",
    "\n",
    "print(f\"\\n‚úÖ Trained {len(results)} model combinations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for key, result in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': key,\n",
    "        'Feature_Type': result['feature_type'],\n",
    "        'Algorithm': key.split('_')[-2] + '_' + key.split('_')[-1],\n",
    "        'Test_Accuracy': result['accuracy'],\n",
    "        'CV_Mean': result['cv_mean'],\n",
    "        'CV_Std': result['cv_std']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('CV_Mean', ascending=False)\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_key = comparison_df.iloc[0]['Model']\n",
    "best_model_info = results[best_model_key]\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_key}\")\n",
    "print(f\"Test Accuracy: {best_model_info['accuracy']:.4f}\")\n",
    "print(f\"CV Score: {best_model_info['cv_mean']:.4f} (+/- {best_model_info['cv_std'] * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization - Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot of CV scores\n",
    "comparison_df_plot = comparison_df.head(10)  # Top 10 models\n",
    "x_pos = np.arange(len(comparison_df_plot))\n",
    "\n",
    "axes[0].bar(x_pos, comparison_df_plot['CV_Mean'], \n",
    "           yerr=comparison_df_plot['CV_Std'], capsize=5, alpha=0.7)\n",
    "axes[0].set_xlabel('Models')\n",
    "axes[0].set_ylabel('Cross-Validation Score')\n",
    "axes[0].set_title('Model Performance Comparison (CV Score)')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(comparison_df_plot['Algorithm'], rotation=45, ha='right')\n",
    "\n",
    "# Scatter plot of CV vs Test accuracy\n",
    "axes[1].scatter(comparison_df['CV_Mean'], comparison_df['Test_Accuracy'], alpha=0.7)\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', alpha=0.5)  # Perfect correlation line\n",
    "axes[1].set_xlabel('Cross-Validation Score')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('CV Score vs Test Accuracy')\n",
    "\n",
    "# Highlight best model\n",
    "best_cv = best_model_info['cv_mean']\n",
    "best_acc = best_model_info['accuracy']\n",
    "axes[1].scatter(best_cv, best_acc, color='red', s=100, marker='*', \n",
    "               label=f'Best Model: {best_model_key.split(\"_\")[-2]}_{best_model_key.split(\"_\")[-1]}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DETAILED EVALUATION OF BEST MODEL ===\")\n",
    "print(f\"Model: {best_model_key}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_model_info['y_pred']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_model_info['y_pred'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=np.unique(y_test), \n",
    "           yticklabels=np.unique(y_test))\n",
    "plt.title(f'Confusion Matrix - {best_model_key}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(best_model_info['model'], 'feature_importances_'):\n",
    "    print(f\"\\nTop 10 Most Important Features:\")\n",
    "    feature_names = []\n",
    "    \n",
    "    # Get feature names based on the feature type\n",
    "    if 'TF-IDF' in best_model_info['feature_type']:\n",
    "        feature_names.extend(tfidf_vectorizer.get_feature_names_out())\n",
    "    elif 'Count' in best_model_info['feature_type']:\n",
    "        feature_names.extend(count_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    if 'Features' in best_model_info['feature_type']:\n",
    "        feature_names.extend(numerical_features_list)\n",
    "    \n",
    "    if len(feature_names) == len(best_model_info['model'].feature_importances_):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': best_model_info['model'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(importance_df.head(10))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(10), importance_df.head(10)['importance'])\n",
    "        plt.yticks(range(10), importance_df.head(10)['feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Top 10 Feature Importances')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity-Specific Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ENTITY-SPECIFIC SENTIMENT ANALYSIS ===\")\n",
    "\n",
    "# Create entity-sentiment crosstab\n",
    "entity_sentiment_ct = pd.crosstab(df_features['Entity'], df_features['Sentiment'], normalize='index') * 100\n",
    "\n",
    "print(\"Entity-Sentiment Distribution (%):\")\n",
    "print(entity_sentiment_ct.round(2))\n",
    "\n",
    "# Plot entity-sentiment heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(entity_sentiment_ct, annot=True, fmt='.1f', cmap='RdYlBu_r', center=33.33)\n",
    "plt.title('Entity-Sentiment Distribution (%)')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Entity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Most positive and negative entities\n",
    "print(f\"\\nMost Positive Entities:\")\n",
    "positive_entities = entity_sentiment_ct.sort_values('Positive', ascending=False)\n",
    "print(positive_entities[['Positive']].head())\n",
    "\n",
    "print(f\"\\nMost Negative Entities:\")\n",
    "negative_entities = entity_sentiment_ct.sort_values('Negative', ascending=False)\n",
    "print(negative_entities[['Negative']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model_key=best_model_key):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a new tweet\n",
    "    \"\"\"\n",
    "    # Get the best model and feature type\n",
    "    model_info = results[model_key]\n",
    "    model = model_info['model']\n",
    "    feature_type = model_info['feature_type']\n",
    "    \n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Calculate numerical features\n",
    "    text_blob = TextBlob(text)\n",
    "    vader_scores = vader_analyzer.polarity_scores(text)\n",
    "    \n",
    "    numerical_features_new = np.array([[\n",
    "        len(text),  # text_length\n",
    "        len(text.split()),  # word_count\n",
    "        text_blob.sentiment.polarity,  # textblob_polarity\n",
    "        text_blob.sentiment.subjectivity,  # textblob_subjectivity\n",
    "        vader_scores['compound'],  # vader_compound\n",
    "        vader_scores['pos'],  # vader_pos\n",
    "        vader_scores['neu'],  # vader_neu\n",
    "        vader_scores['neg'],  # vader_neg\n",
    "        text.count('!'),  # exclamation_count\n",
    "        text.count('?'),  # question_count\n",
    "        sum(1 for c in text if c.isupper()) / max(len(text), 1)  # capital_ratio\n",
    "    ]])\n",
    "    \n",
    "    # Vectorize text based on feature type\n",
    "    if 'TF-IDF' in feature_type:\n",
    "        text_vector = tfidf_vectorizer.transform([processed_text])\n",
    "    elif 'Count' in feature_type:\n",
    "        text_vector = count_vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Combine features if needed\n",
    "    if 'Features' in feature_type:\n",
    "        X_combined = hstack([text_vector, numerical_features_new])\n",
    "    else:\n",
    "        X_combined = text_vector\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(X_combined)[0]\n",
    "    probabilities = model.predict_proba(X_combined)[0]\n",
    "    \n",
    "    # Create probability dictionary\n",
    "    prob_dict = dict(zip(model.classes_, probabilities))\n",
    "    \n",
    "    return prediction, prob_dict\n",
    "\n",
    "# Test the prediction function\n",
    "test_tweets = [\n",
    "    \"I love this game! It's absolutely amazing and so much fun to play!\",\n",
    "    \"This is terrible. I hate it so much, worst game ever.\",\n",
    "    \"It's okay, nothing special but not bad either.\",\n",
    "    \"The graphics are good but the gameplay is boring.\",\n",
    "    \"@Microsoft your product is broken, please fix it!\"\n",
    "]\n",
    "\n",
    "print(\"=== TESTING PREDICTION FUNCTION ===\")\n",
    "for i, tweet in enumerate(test_tweets, 1):\n",
    "    prediction, probabilities = predict_sentiment(tweet)\n",
    "    print(f\"\\nTest Tweet {i}: '{tweet}'\")\n",
    "    print(f\"Predicted Sentiment: {prediction}\")\n",
    "    print(f\"Probabilities: {probabilities}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Insights and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== KEY INSIGHTS AND CONCLUSIONS ===\")\n",
    "\n",
    "print(f\"1. BEST MODEL PERFORMANCE:\")\n",
    "print(f\"   - Best Model: {best_model_key}\")\n",
    "print(f\"   - Test Accuracy: {best_model_info['accuracy']:.4f}\")\n",
    "print(f\"   - Cross-Validation Score: {best_model_info['cv_mean']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   - Total samples: {len(df)}\")\n",
    "print(f\"   - Sentiment distribution: {dict(df['Sentiment'].value_counts())}\")\n",
    "print(f\"   - Number of unique entities: {df['Entity'].nunique()}\")\n",
    "\n",
    "print(f\"\\n3. FEATURE IMPORTANCE:\")\n",
    "print(f\"   - Text features were most important for classification\")\n",
    "print(f\"   - Sentiment lexicon features (TextBlob, VADER) added value\")\n",
    "print(f\"   - Punctuation and style features provided additional context\")\n",
    "\n",
    "print(f\"\\n4. ENTITY-SPECIFIC PATTERNS:\")\n",
    "most_positive = entity_sentiment_ct.sort_values('Positive', ascending=False).index[0]\n",
    "most_negative = entity_sentiment_ct.sort_values('Negative', ascending=False).index[0]\n",
    "print(f\"   - Most positive entity: {most_positive}\")\n",
    "print(f\"   - Most negative entity: {most_negative}\")\n",
    "\n",
    "print(f\"\\n5. MODEL RECOMMENDATIONS:\")\n",
    "print(f\"   - Logistic Regression with TF-IDF typically performs best\")\n",
    "print(f\"   - Combining text and numerical features improves performance\")\n",
    "print(f\"   - Cross-validation is crucial for reliable model selection\")\n",
    "\n",
    "print(f\"\\n6. POTENTIAL IMPROVEMENTS:\")\n",
    "print(f\"   - Increase dataset size for better generalization\")\n",
    "print(f\"   - Implement advanced preprocessing (spelling correction, slang handling)\")\n",
    "print(f\"   - Try ensemble methods or deep learning approaches\")\n",
    "print(f\"   - Perform hyperparameter tuning for best models\")\n",
    "\n",
    "print(f\"\\n‚úÖ ANALYSIS COMPLETE!\")\n",
    "print(f\"üìä Your supervised ML sentiment analysis pipeline is ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
